>>> from pyspark.sql import SparkSession
>>> from pyspark.sql import functions as F
>>> from pyspark.sql.types import StringType, StructType
>>>
>>> # spark = SparkSession.builder.appName("cassandra_spark_app").getOrCreate()
...
>>> spark.conf.set("spark.cassandra.connection.host", "localhost")
>>> checkpoint_location = "tmp/orders_checkpoint"
>>>
>>> keyspace = "streaming_student782_3"
>>>
>>> # Console output
... def console_output(df, freq):
...     from datetime import datetime as dt
...     date = dt.now().strftime("%Y%m%d%H%M%S")
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime='%s seconds' % freq) \
...         .option("checkpointLocation", checkpoint_location + "/" + date) \
...         .options(truncate=False) \
...         .start()
...
>>> # orders
... kafka_brokers = "bigdataanalytics2-worker-shdpt-v31-1-0:6667"
>>> kafka_topic = "lesson6_student782_3_orders_topic_json"
>>>
>>> raw_orders = spark.readStream. \
...     format("kafka"). \
...     option("kafka.bootstrap.servers", kafka_brokers). \
...     option("subscribe", kafka_topic). \
...     option("maxOffsetsPerTrigger", "20"). \
...     option("startingOffsets", "earliest"). \
...     load()
>>>
>>> schema = StructType() \
...     .add("order_id", StringType()) \
...     .add("customer_id", StringType()) \
...     .add("order_status", StringType()) \
...     .add("order_purchase_timestamp", StringType()) \
...     .add("order_approved_at", StringType()) \
...     .add("order_delivered_carrier_date", StringType()) \
...     .add("order_delivered_customer_date", StringType()) \
...     .add("order_estimated_delivery_date", StringType())
>>>
>>> parsed_orders = raw_orders \
...     .select(F.from_json(F.col("value").cast("String"), schema).alias("value"), "offset") \
...     .select("value.*")
>>>
>>> stream = console_output(parsed_orders, 10)
21/08/15 06:14:51 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|order_id                        |customer_id                     |order_status|order_purchase_timestamp|order_approved_at  |order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|53cdb2fc8bc7dce0b6741e2150273451|b0830fb4747a6c6d20dea0b8c802d7ef|delivered   |2018-07-24 20:41:37     |2018-07-26 03:24:27|2018-07-26 14:31:00         |2018-08-07 15:27:45          |2018-08-13 00:00:00          |
|76c6e866289321a7c93b82b54852dc33|f54a9f0e6b351c431402b8461ea51999|delivered   |2017-01-23 18:29:09     |2017-01-25 02:50:47|2017-01-26 14:16:31         |2017-02-02 14:08:10          |2017-03-06 00:00:00          |
|e69bfb5eb88e0ed6a785585b27e16dbf|31ad1d1b63eb9962463f764d4e6e0c9d|delivered   |2017-07-29 11:55:02     |2017-07-29 12:05:32|2017-08-10 19:45:24         |2017-08-16 17:14:30          |2017-08-23 00:00:00          |
|34513ce0c4fab462a55830c0989c7edb|7711cf624183d843aafe81855097bc37|delivered   |2017-07-13 19:58:11     |2017-07-13 20:10:08|2017-07-14 18:43:29         |2017-07-19 14:04:48          |2017-08-08 00:00:00          |
|116f0b09343b49556bbad5f35bee0cdf|3187789bec990987628d7a9beb4dd6ac|delivered   |2017-12-26 23:41:31     |2017-12-26 23:50:22|2017-12-28 18:33:05         |2018-01-08 22:36:36          |2018-01-29 00:00:00          |
|989225ba6d0ebd5873335f7e01de2ae7|816f8653d5361cbf94e58c33f2502a5c|delivered   |2017-12-12 13:56:04     |2017-12-14 13:54:13|2017-12-16 00:18:57         |2018-01-03 18:03:36          |2018-01-08 00:00:00          |
|e481f51cbdc54678b7cc49136f2d6af7|9ef432eb6251297304e76186b10a928d|delivered   |2017-10-02 10:56:33     |2017-10-02 11:07:15|2017-10-04 19:55:00         |2017-10-10 21:25:13          |2017-10-18 00:00:00          |
|47770eb9100c2d0c44946d9cf07ec65d|41ce2a54c0b03bf3443c3d931a367089|delivered   |2018-08-08 08:38:49     |2018-08-08 08:55:23|2018-08-08 13:50:00         |2018-08-17 18:06:29          |2018-09-04 00:00:00          |
|949d5b44dbf5de918fe9c16f97b45f8a|f88197465ea7920adcdbec7375364d82|delivered   |2017-11-18 19:28:06     |2017-11-18 19:45:59|2017-11-22 13:39:59         |2017-12-02 00:28:42          |2017-12-15 00:00:00          |
|e6ce16cb79ec1d90b1da9085a6118aeb|494dded5b201313c64ed7f100595b95c|delivered   |2017-05-16 19:41:10     |2017-05-16 19:50:18|2017-05-18 11:40:40         |2017-05-29 11:18:31          |2017-06-07 00:00:00          |
|82566a660a982b15fb86e904c8d32918|d3e3b74c766bc6214e0c830b17ee2341|delivered   |2018-06-07 10:06:19     |2018-06-09 03:13:12|2018-06-11 13:29:00         |2018-06-19 12:05:52          |2018-07-18 00:00:00          |
|432aaf21d85167c2c86ec9448c4e42cc|3df704f53d3f1d4818840b34ec672a9f|delivered   |2018-03-01 14:14:28     |2018-03-01 15:10:47|2018-03-02 21:09:20         |2018-03-12 23:36:26          |2018-03-21 00:00:00          |
|dcb36b511fcac050b97cd5c05de84dc3|3b6828a50ffe546942b7a473d70ac0fc|delivered   |2018-06-07 19:03:12     |2018-06-12 23:31:02|2018-06-11 14:54:00         |2018-06-21 15:34:32          |2018-07-04 00:00:00          |
|ad21c59c0840e6cb83a9ceb5573f8159|8ab97904e6daea8866dbdbc4fb7aad2c|delivered   |2018-02-13 21:18:39     |2018-02-13 22:20:29|2018-02-14 19:46:34         |2018-02-16 18:17:02          |2018-02-26 00:00:00          |
|a4591c265e18cb1dcee52889e2d8acc3|503740e9ca751ccdda7ba28e9ab8f608|delivered   |2017-07-09 21:57:05     |2017-07-09 22:10:13|2017-07-11 14:58:04         |2017-07-26 10:57:55          |2017-08-01 00:00:00          |
|136cce7faa42fdb2cefd53fdc79a6098|ed0271e0b7da060a393796590e7b737a|invoiced    |2017-04-11 12:22:08     |2017-04-13 13:25:17|                            |                             |2017-05-09 00:00:00          |
|6514b8ad8028c9f2cc2374ded245783f|9bdf08b4b3b52b5526ff42d37d47f222|delivered   |2017-05-16 13:10:30     |2017-05-16 13:22:11|2017-05-22 10:07:46         |2017-05-26 12:55:51          |2017-06-07 00:00:00          |
|5ff96c15d0b717ac6ad1f3d77225a350|19402a48fe860416adf93348aba37740|delivered   |2018-07-25 17:44:10     |2018-07-25 17:55:14|2018-07-26 13:16:00         |2018-07-30 15:52:25          |2018-08-08 00:00:00          |
|85ce859fd6dc634de8d2f1e290444043|059f7fc5719c7da6cbafe370971a8d70|delivered   |2017-11-21 00:03:41     |2017-11-21 00:14:22|2017-11-23 21:32:26         |2017-11-27 18:28:00          |2017-12-11 00:00:00          |
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|order_id                        |customer_id                     |order_status|order_purchase_timestamp|order_approved_at  |order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|434d158e96bdd6972ad6e6d73ddcfd22|2a1dfb647f32f4390e7b857c67458536|delivered   |2018-06-01 12:23:13     |2018-06-05 03:35:15|2018-06-08 11:49:00         |2018-06-18 21:32:52          |2018-07-17 00:00:00          |
|948097deef559c742e7ce321e5e58919|8644be24d48806bc3a88fd59fb47ceb1|delivered   |2017-08-04 17:10:39     |2017-08-04 17:25:11|2017-08-07 17:52:01         |2017-08-12 14:08:40          |2017-09-01 00:00:00          |
|ee64d42b8cf066f35eac1cf57de1aa85|caded193e8e47b8362864762a83db3c5|shipped     |2018-06-04 16:44:48     |2018-06-05 04:31:18|2018-06-05 14:32:00         |                             |2018-06-28 00:00:00          |
|d17dc4a904426827ca80f2ccb3a6be56|569cf68214806a39acc0f39344aea67f|delivered   |2017-05-14 20:28:25     |2017-05-14 20:42:45|2017-05-16 08:17:46         |2017-05-25 09:14:31          |2017-06-12 00:00:00          |
|138849fd84dff2fb4ca70a0a34c4aa1c|9b18f3fc296990b97854e351334a32f6|delivered   |2018-02-01 14:02:19     |2018-02-03 02:53:07|2018-02-06 19:13:26         |2018-02-14 13:41:59          |2018-02-23 00:00:00          |
|47aa4816b27ba60ec948cd019cc1afc1|148348ff65384b4249b762579532e248|delivered   |2018-06-26 13:42:52     |2018-06-27 08:35:32|2018-06-27 13:20:00         |2018-07-03 18:37:46          |2018-07-20 00:00:00          |
|403b97836b0c04a622354cf531062e5f|738b086814c6fcc74b8cc583f8516ee3|delivered   |2018-01-02 19:00:43     |2018-01-02 19:09:04|2018-01-03 18:19:09         |2018-01-20 01:38:59          |2018-02-06 00:00:00          |
|203096f03d82e0dffbc41ebc2e2bcfb7|d2b091571da224a1b36412c18bc3bbfe|delivered   |2017-09-18 14:31:30     |2017-09-19 04:04:09|2017-10-06 17:50:03         |2017-10-09 22:23:46          |2017-09-28 00:00:00          |
|f848643eec1d69395095eb3840d2051e|4fa1cd166fa598be6de80fa84eaade43|delivered   |2018-03-15 08:52:40     |2018-03-15 09:09:31|2018-03-15 19:52:48         |2018-03-19 18:08:32          |2018-03-29 00:00:00          |
|2807d0e504d6d4894d41672727bc139f|72ae281627a6102d9b3718528b420f8a|delivered   |2018-02-03 20:37:35     |2018-02-03 20:50:22|2018-02-05 22:37:28         |2018-02-08 16:13:46          |2018-02-21 00:00:00          |
|95266dbfb7e20354baba07964dac78d5|a166da34890074091a942054b36e4265|delivered   |2018-01-08 07:55:29     |2018-01-08 08:07:31|2018-01-24 23:16:37         |2018-01-26 17:32:38          |2018-02-21 00:00:00          |
|fbf9ac61453ac646ce8ad9783d7d0af6|3a874b4d4c4b6543206ff5d89287f0c3|delivered   |2018-02-20 23:46:53     |2018-02-22 02:30:46|2018-02-26 22:25:22         |2018-03-21 22:03:54          |2018-03-12 00:00:00          |
|acce194856392f074dbf9dada14d8d82|7e20bf5ca92da68200643bda76c504c6|delivered   |2018-06-04 00:00:13     |2018-06-05 00:35:10|2018-06-05 13:24:00         |2018-06-16 15:20:55          |2018-07-18 00:00:00          |
|83018ec114eee8641c97e08f7b4e926f|7f8c8b9c2ae27bf3300f670c3d478be8|delivered   |2017-10-26 15:54:26     |2017-10-26 16:08:14|2017-10-26 21:46:53         |2017-11-08 22:22:00          |2017-11-23 00:00:00          |
|f3e7c359154d965827355f39d6b1fdac|62b423aab58096ca514ba6aa06be2f98|delivered   |2018-08-09 11:44:40     |2018-08-10 03:24:51|2018-08-10 12:29:00         |2018-08-13 18:24:27          |2018-08-17 00:00:00          |
|dd78f560c270f1909639c11b925620ea|8b212b9525f9e74e85e37ed6df37693e|delivered   |2018-03-12 01:50:26     |2018-03-12 03:28:34|2018-03-12 21:06:37         |2018-03-21 14:41:50          |2018-03-28 00:00:00          |
|ecab90c9933c58908d3d6add7c6f5ae3|761df82feda9778854c6dafdaeb567e4|delivered   |2018-02-25 13:50:30     |2018-02-25 14:47:35|2018-02-26 22:28:50         |2018-03-27 23:29:14          |2018-04-13 00:00:00          |
|1790eea0b567cf50911c057cf20f90f9|52142aa69d8d0e1247ab0cada0f76023|delivered   |2018-04-16 21:15:39     |2018-04-16 22:10:26|2018-04-18 13:05:09         |2018-05-05 12:28:34          |2018-05-15 00:00:00          |
|d887b52c6516beb39e8cd44a5f8b60f7|d9ef95f98d8da3b492bb8c0447910498|delivered   |2018-02-03 12:38:58     |2018-02-03 12:50:30|2018-02-05 21:26:53         |2018-02-22 00:07:55          |2018-03-07 00:00:00          |
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+

stream.stop()
>>> customer_names = spark.read \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .load()

>>>
>>> customer_names.printSchema()
root
 |-- cid: string (nullable = true)
 |-- full_name: string (nullable = true)

>>> customer_names.show(truncate=False)
+--------------------------------+----------------+
|cid                             |full_name       |
+--------------------------------+----------------+
|55490de47a9c2abff0a27a5296ae9c2c|Terrance Trump  |
|ffdc2af7160aed3bad21cbc8ec68c0c5|Philip Clegg    |
|d4faa220408c20e53595d2950f361f3b|May Pollock     |
|59cf1a479d104e38587b9fe9d60febe3|Michael Dillon  |
|db925a533d94cee42707026949892c7e|Richard Beere   |
|48c83ab1910a8d95f8702dde180d658b|Martina Moore   |
|6eae55140163e2af86907a15a478f2d2|Caroline Brown  |
|4fd75fb5ef1f01c7585bf746092a1544|Andrew Booth    |
|2c5491a76ec0a56db58282ea7ae70617|David Matthews  |
|4d833833ec5cc5bb9e6c230820e4bc54|Tracey Groves   |
|fac60d7c4df3896a5af5db6c91e03797|Jane Gill       |
|aa3d8c9eec8e52e90b117efe5e97a560|Lachlan McLaren |
|686ca7499141a82f95123c370af061b0|Ibrar Majid     |
|2b47ff70d422cc3ada976dea40f48782|David Francis   |
|5072cf2f4cbec30b8ba917d5d7b6b125|Michal Buras    |
|20b5aae6a3e31111009f9a7ecc31a232|Ann Blurton     |
|bf141bf67fbe428d558bcf0e018eab60|Michael McGrane |
|e40265c6ceebf38ea830b695d9340bda|Sharon Lambe    |
|6c347ef65dd574fb9c2d654c3650fc43|Archie MacGregor|
|7e016f9ea275279784f42e0642214284|Maurice Savage  |
+--------------------------------+----------------+
only showing top 20 rows

>>> orders_by_names = parsed_orders\
...     .join(customer_names, parsed_orders.customer_id == customer_names.cid, "inner")\
...     .select("order_id", customer_names.cid, customer_names.full_name)\
...     .withColumnRenamed("order_id", "oid")
>>>
>>>
>>> stream = console_output(orders_by_names, 10)
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------------------+--------------------------------+-------------+
|oid                             |cid                             |full_name    |
+--------------------------------+--------------------------------+-------------+
|76c6e866289321a7c93b82b54852dc33|f54a9f0e6b351c431402b8461ea51999|George Rae   |
|e481f51cbdc54678b7cc49136f2d6af7|9ef432eb6251297304e76186b10a928d|Kevin Maguire|
|5ff96c15d0b717ac6ad1f3d77225a350|19402a48fe860416adf93348aba37740|Wanda Woods  |
+--------------------------------+--------------------------------+-------------+

-------------------------------------------
Batch: 1
-------------------------------------------
+---+---+---------+
|oid|cid|full_name|
+---+---+---------+
+---+---+---------+

-------------------------------------------
Batch: 2
-------------------------------------------
+--------------------------------+--------------------------------+-------------+
|oid                             |cid                             |full_name    |
+--------------------------------+--------------------------------+-------------+
|9faeb9b2746b9d7526aef5acb08e2aa0|79183cd650e2bb0d475b0067d45946ac|Michael Dixon|
|2edfd6d1f0b4cd0db4bf37b1b224d855|241e78de29b3090cfa1b5d73a8130c72|Russell Carr |
+--------------------------------+--------------------------------+-------------+

stream.stop()
21/08/15 06:48:30 ERROR streaming.MicroBatchExecution: Query [id = bd1c2ae4-335f-452f-9d22-9f7343d59e44, runId = 01f43e75-66b6-48ea-a116-c36d0874a9c7] terminated with error
java.io.IOException: Failed to open native connection to Cassandra at {127.0.0.1}:9042
        at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:168)
        at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)
        at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)
        at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)
        at com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)
        at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)
        at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:79)
        at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)
        at com.datastax.spark.connector.cql.CassandraConnector.withClusterDo(CassandraConnector.scala:122)
        at org.apache.spark.sql.cassandra.CassandraSourceRelation.predicatePushDown(CassandraSourceRelation.scala:124)
        at org.apache.spark.sql.cassandra.CassandraSourceRelation.unhandledFilters(CassandraSourceRelation.scala:96)
        at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.selectFilters(DataSourceStrategy.scala:557)
        at org.apache.spark.sql.execution.datasources.DataSourceStrategy.pruneFilterProjectRaw(DataSourceStrategy.scala:358)
        at org.apache.spark.sql.execution.datasources.DataSourceStrategy.pruneFilterProject(DataSourceStrategy.scala:321)
        at org.apache.spark.sql.execution.datasources.DataSourceStrategy.apply(DataSourceStrategy.scala:289)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:63)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:63)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
        at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
        at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
        at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:78)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:75)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
        at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:75)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:67)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)
        at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)
        at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)
        at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)
        at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$4.apply(MicroBatchExecution.scala:528)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$4.apply(MicroBatchExecution.scala:519)
        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:519)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
        at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
        at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.DriverException: Connection thread interrupted))
        at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:268)
        at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:107)
        at com.datastax.driver.core.Cluster$Manager.negotiateProtocolVersionAndConnect(Cluster.java:1652)
        at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1571)
        at com.datastax.driver.core.Cluster.getMetadata(Cluster.java:451)
        at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:161)
        ... 91 more
>>> stream.stop()
>>> def cassandra_output(df, freq):
...     from datetime import datetime as dt
...     date = dt.now().strftime("%Y%m%d%H%M%S")
...     df.writeStream \
...         .trigger(processingTime='%s seconds' % freq) \
...         .format("org.apache.spark.sql.cassandra") \
...         .outputMode("update")\
...         .options(keyspace=keyspace, table="customer_by_order_id") \
...         .option("checkpointLocation", checkpoint_location + "/" + date)\
...         .start()
...
>>> stream = cassandra_output(orders_by_names, 10)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 9, in cassandra_output
  File "/spark2.4/python/pyspark/sql/streaming.py", line 1109, in start
    return self._sq(self._jwrite.start())
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/spark2.4/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o100.start.
: java.lang.UnsupportedOperationException: Data source org.apache.spark.sql.cassandra does not support streamed writing
        at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:311)
        at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:322)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)

>>> def foreach_batch(df, epoch):
...     df.write\
...         .format("org.apache.spark.sql.cassandra") \
...         .mode("append")\
...         .options(keyspace=keyspace, table="customer_by_order_id") \
...         .save()
...
>>> def cassandra_output_batch(df, freq):
...     from datetime import datetime as dt
...     date = dt.now().strftime("%Y%m%d%H%M%S")
...     return orders_by_names\
...         .writeStream\
...         .trigger(processingTime='%s seconds' % 10) \
...         .foreachBatch(foreach_batch)\
...         .option("checkpointLocation", checkpoint_location + "/" + date)\
...         .start()
...
>>> stream = cassandra_output_batch(orders_by_names, 10)
>>> stream.stop()
>>> names_df = spark.sql("""select '20b5aae6a3e31111009f9a7ecc31a232' as cid, 'Ann Peterson 2' as full_name""")
>>> names_df.show()
+--------------------+--------------+
|                 cid|     full_name|
+--------------------+--------------+
|20b5aae6a3e311110...|Ann Peterson 2|
+--------------------+--------------+

>>> names_df.write \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .mode("append") \
...     .save()
>>>
>>> # overwrite
... names_df.write \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .mode("overwrite") \
...     .save()
Traceback (most recent call last):
  File "<stdin>", line 5, in <module>
  File "/spark2.4/python/pyspark/sql/readwriter.py", line 737, in save
    self._jwrite.save()
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/spark2.4/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o157.save.
: java.lang.UnsupportedOperationException: You are attempting to use overwrite mode which will truncate
this table prior to inserting data. If you would merely like
to change data already in the table use the "Append" mode.
To actually truncate please pass in true value to the option
"confirm.truncate" when saving.
        at org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:69)
        at org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:87)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
        at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)

>>>
>>>
>>> # read all names
... all_names_df = spark.read \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .load()
>>> all_names_df.show()
+--------------------+----------------+
|                 cid|       full_name|
+--------------------+----------------+
|55490de47a9c2abff...|  Terrance Trump|
|ffdc2af7160aed3ba...|    Philip Clegg|
|d4faa220408c20e53...|     May Pollock|
|59cf1a479d104e385...|  Michael Dillon|
|db925a533d94cee42...|   Richard Beere|
|48c83ab1910a8d95f...|   Martina Moore|
|6eae55140163e2af8...|  Caroline Brown|
|4fd75fb5ef1f01c75...|    Andrew Booth|
|2c5491a76ec0a56db...|  David Matthews|
|4d833833ec5cc5bb9...|   Tracey Groves|
|fac60d7c4df3896a5...|       Jane Gill|
|aa3d8c9eec8e52e90...| Lachlan McLaren|
|686ca7499141a82f9...|     Ibrar Majid|
|2b47ff70d422cc3ad...|   David Francis|
|5072cf2f4cbec30b8...|    Michal Buras|
|20b5aae6a3e311110...|  Ann Peterson 2|
|bf141bf67fbe428d5...| Michael McGrane|
|e40265c6ceebf38ea...|    Sharon Lambe|
|6c347ef65dd574fb9...|Archie MacGregor|
|7e016f9ea27527978...|  Maurice Savage|
+--------------------+----------------+
only showing top 20 rows

>>> c_name_df = all_names_df.filter(F.col("cid") == "20b5aae6a3e31111009f9a7ecc31a232")
>>> c_name_df.show()
+--------------------+--------------+
|                 cid|     full_name|
+--------------------+--------------+
|20b5aae6a3e311110...|Ann Peterson 2|
+--------------------+--------------+

>>> c_name_df.count()
1
>>> jane_df = all_names_df.filter(F.col("full_name") == "Ann Peterson")
>>> jane_df.show()  # only first 20
+---+---------+
|cid|full_name|
+---+---------+
+---+---------+

>>> jane_df.count()  # all records
0
>>> c_name_df.explain(True)
== Parsed Logical Plan ==
'Filter ('cid = 20b5aae6a3e31111009f9a7ecc31a232)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter (cid#390 = 20b5aae6a3e31111009f9a7ecc31a232)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Optimized Logical Plan ==
Filter (isnotnull(cid#390) && (cid#390 = 20b5aae6a3e31111009f9a7ecc31a232))
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Physical Plan ==
*(1) Filter isnotnull(cid#390)
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6 [cid#390,full_name#391] PushedFilters: [IsNotNull(cid), *EqualTo(cid,20b5aae6a3e31111009f9a7ecc31a232)], ReadSchema: struct<cid:string,full_name:string>
>>> jane_df.explain(True)
== Parsed Logical Plan ==
'Filter ('full_name = Ann Peterson)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter (full_name#391 = Ann Peterson)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Optimized Logical Plan ==
Filter (isnotnull(full_name#391) && (full_name#391 = Ann Peterson))
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Physical Plan ==
*(1) Filter (isnotnull(full_name#391) && (full_name#391 = Ann Peterson))
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6 [cid#390,full_name#391] PushedFilters: [IsNotNull(full_name), EqualTo(full_name,Ann Peterson)], ReadSchema: struct<cid:string,full_name:string>
>>> between_select = all_names_df.filter(F.col("cid").between('20b5aae6a3e31111009f9a7ecc31a232', 'b89010d4a6acaa06d4ef89043869838e'))
>>> between_select.explain(True)
== Parsed Logical Plan ==
'Filter (('cid >= 20b5aae6a3e31111009f9a7ecc31a232) && ('cid <= b89010d4a6acaa06d4ef89043869838e))
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter ((cid#390 >= 20b5aae6a3e31111009f9a7ecc31a232) && (cid#390 <= b89010d4a6acaa06d4ef89043869838e))
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Optimized Logical Plan ==
Filter ((isnotnull(cid#390) && (cid#390 >= 20b5aae6a3e31111009f9a7ecc31a232)) && (cid#390 <= b89010d4a6acaa06d4ef89043869838e))
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Physical Plan ==
*(1) Filter ((isnotnull(cid#390) && (cid#390 >= 20b5aae6a3e31111009f9a7ecc31a232)) && (cid#390 <= b89010d4a6acaa06d4ef89043869838e))
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6 [cid#390,full_name#391] PushedFilters: [IsNotNull(cid), GreaterThanOrEqual(cid,20b5aae6a3e31111009f9a7ecc31a232), LessThanOrEqual(cid,b8..., ReadSchema: struc
t<cid:string,full_name:string>
>>> between_select.show()
+--------------------+----------------+
|                 cid|       full_name|
+--------------------+----------------+
|55490de47a9c2abff...|  Terrance Trump|
|59cf1a479d104e385...|  Michael Dillon|
|48c83ab1910a8d95f...|   Martina Moore|
|6eae55140163e2af8...|  Caroline Brown|
|4fd75fb5ef1f01c75...|    Andrew Booth|
|2c5491a76ec0a56db...|  David Matthews|
|4d833833ec5cc5bb9...|   Tracey Groves|
|aa3d8c9eec8e52e90...| Lachlan McLaren|
|686ca7499141a82f9...|     Ibrar Majid|
|2b47ff70d422cc3ad...|   David Francis|
|5072cf2f4cbec30b8...|    Michal Buras|
|20b5aae6a3e311110...|  Ann Peterson 2|
|6c347ef65dd574fb9...|Archie MacGregor|
|7e016f9ea27527978...|  Maurice Savage|
|ae2164e850f39dce4...|  Mohammed Hoque|
|4ad5a269a2d59d2c8...|   Amy Gillespie|
|90d2e6f72916e7282...|     Edward Lees|
|72600f002ba1550d6...|    Gordon Cutts|
|2fdffca8dcdf01547...|    John Collier|
|a00009bf8489ae779...| Simon Broomhead|
+--------------------+----------------+
only showing top 20 rows

>>> between_select.count()
71
>>> in_select = all_names_df.filter(F.col("cid").isin('20b5aae6a3e31111009f9a7ecc31a232', 'b89010d4a6acaa06d4ef89043869838e'))
>>> in_select.explain(True)
== Parsed Logical Plan ==
'Filter 'cid IN (20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter cid#390 IN (20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Optimized Logical Plan ==
Filter cid#390 IN (20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e)
+- Relation[cid#390,full_name#391] org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6

== Physical Plan ==
*(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7bf8e2c6 [cid#390,full_name#391] PushedFilters: [*In(cid, [20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e])], ReadSchema: struct<cid:string,full_name:s
tring>
>>> in_select.show()
+--------------------+--------------+
|                 cid|     full_name|
+--------------------+--------------+
|20b5aae6a3e311110...|Ann Peterson 2|
|b89010d4a6acaa06d...|  Stephen Wood|
+--------------------+--------------+

>>> in_select.count()
21/08/15 07:03:16 WARN core.RequestHandler: Query '[2 bound values] SELECT count(*) FROM "streaming_student782_3"."customer_names" WHERE "cid" IN (?, ?)   ALLOW FILTERING;' generated server side warning(s): Aggregation query used on mu
ltiple partition keys (IN restriction)
2
>>> cass_big_df = spark.read \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(table="users_many", keyspace="keyspace1") \
...     .load()
>>>
>>> between_select = cass_big_df.filter(F.col("user_id").between(4164237664, 4164237664+10) )
>>> between_select.explain(True)
== Parsed Logical Plan ==
'Filter (('user_id >= 4164237664) && ('user_id <= 4164237674))
+- Relation[user_id#457,gender#458] org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484

== Analyzed Logical Plan ==
user_id: string, gender: string
Filter ((cast(user_id#457 as bigint) >= 4164237664) && (cast(user_id#457 as bigint) <= 4164237674))
+- Relation[user_id#457,gender#458] org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484

== Optimized Logical Plan ==
Filter ((isnotnull(user_id#457) && (cast(user_id#457 as bigint) >= 4164237664)) && (cast(user_id#457 as bigint) <= 4164237674))
+- Relation[user_id#457,gender#458] org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484

== Physical Plan ==
*(1) Filter (((cast(user_id#457 as bigint) >= 4164237664) && (cast(user_id#457 as bigint) <= 4164237674)) && isnotnull(user_id#457))
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484 [user_id#457,gender#458] PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:string,gender:string>
>>> between_select.show()
[Stage 45:>                                                         (0 + 2) / 4]
[Stage 45:=============================>                            (2 + 2) / 4]
+----------+------+
|   user_id|gender|
+----------+------+
|4164237664|     9|
|4164237669|     9|
|4164237665|     9|
|4164237673|     9|
|4164237666|     9|
|4164237667|     9|
|4164237672|     9|
|4164237674|     9|
|4164237670|     9|
|4164237671|     9|
|4164237668|     9|
+----------+------+

>>>
>>>
>>> in_select = cass_big_df.filter(F.col("user_id").isin(4164237664, 4164237664+1) )
>>> in_select.explain(True)
== Parsed Logical Plan ==
'Filter 'user_id IN (4164237664,4164237665)
+- Relation[user_id#457,gender#458] org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484

== Analyzed Logical Plan ==
user_id: string, gender: string
Filter cast(user_id#457 as string) IN (cast(4164237664 as string),cast(4164237665 as string))
+- Relation[user_id#457,gender#458] org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484

== Optimized Logical Plan ==
Filter user_id#457 IN (4164237664,4164237665)
+- Relation[user_id#457,gender#458] org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484

== Physical Plan ==
*(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@400b5484 [user_id#457,gender#458] PushedFilters: [*In(user_id, [4164237664,4164237665])], ReadSchema: struct<user_id:string,gender:string>
>>> in_select.show()
+----------+------+
|   user_id|gender|
+----------+------+
|4164237664|     9|
|4164237665|     9|
+----------+------+

>>>
