[student782_3@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$ /spark2.4/bin/pyspark
Python 2.7.5 (default, Nov 16 2020, 22:23:17)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Warning: Ignoring non-Spark config property: hive.metastore.uris
21/07/28 04:41:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.7
      /_/

Using Python version 2.7.5 (default, Nov 16 2020 22:23:17)
SparkSession available as 'spark'.
>>> from pyspark.sql import SparkSession
>>> spark = SparkSession \
...     .builder \
...     .appName("FileSourceApp") \
...     .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-stream-chkpnt/") \
...     .getOrCreate()
>>> file_df = spark \
...     .readStream \
...     .format("csv") \
...     .schema("key INT, value STRING") \
...     .load("/tmp/spark-stream/*")
21/07/28 04:44:12 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
>>> file_stream = file_df \
...     .writeStream \
...     .format("console") \
...     .start()
>>> file_stream.awaitTermination()

^CTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/spark2.4/python/pyspark/sql/streaming.py", line 103, in awaitTermination
    return self._jsq.awaitTermination()
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1255, in __call__
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 985, in send_command
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1152, in send_command
  File "/usr/lib64/python2.7/socket.py", line 447, in readline
    data = self._sock.recv(self._rbufsize)
  File "/spark2.4/python/pyspark/context.py", line 270, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
