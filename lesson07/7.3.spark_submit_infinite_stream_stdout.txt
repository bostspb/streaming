[student782_3@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$ /spark2.4/bin/spark-submit 7.3.spark_submit_infinite_stream.py
Warning: Ignoring non-Spark config property: hive.metastore.uris
21/08/17 05:40:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/08/17 05:40:41 INFO spark.SparkContext: Running Spark version 2.4.7
21/08/17 05:40:41 INFO spark.SparkContext: Submitted application: spark-submit-infinite-stream-app
21/08/17 05:40:41 INFO spark.SecurityManager: Changing view acls to: student782_3
21/08/17 05:40:41 INFO spark.SecurityManager: Changing modify acls to: student782_3
21/08/17 05:40:41 INFO spark.SecurityManager: Changing view acls groups to:
21/08/17 05:40:41 INFO spark.SecurityManager: Changing modify acls groups to:
21/08/17 05:40:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(student782_3); groups with view permissions: Set(); users  with modify permissions: Set(student
782_3); groups with modify permissions: Set()
21/08/17 05:40:42 INFO util.Utils: Successfully started service 'sparkDriver' on port 46611.
21/08/17 05:40:42 INFO spark.SparkEnv: Registering MapOutputTracker
21/08/17 05:40:42 INFO spark.SparkEnv: Registering BlockManagerMaster
21/08/17 05:40:42 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/08/17 05:40:42 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/08/17 05:40:42 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4d28269b-4ef5-4654-8d84-19bc5a47d2fc
21/08/17 05:40:42 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
21/08/17 05:40:42 INFO spark.SparkEnv: Registering OutputCommitCoordinator
21/08/17 05:40:42 INFO util.log: Logging initialized @2406ms
21/08/17 05:40:42 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
21/08/17 05:40:42 INFO server.Server: Started @2481ms
21/08/17 05:40:42 INFO server.AbstractConnector: Started ServerConnector@241cb450{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21/08/17 05:40:42 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c0e722d{/jobs,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10aa3b68{/jobs/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ae22ed{/jobs/job,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52cae35d{/jobs/job/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@137ae317{/stages,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@316102df{/stages/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5687a7c1{/stages/stage,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e3adaaa{/stages/stage/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4568d903{/stages/pool,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@548efb7{/stages/pool/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3022fd32{/storage,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48b59e16{/storage/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b16f6c2{/storage/rdd,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9619d40{/storage/rdd/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e8d3a33{/environment,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a2c54da{/environment/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36700bbb{/executors,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33fe1ba8{/executors/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74e5e346{/executors/threadDump,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cc7e384{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a5c0674{/static,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@307f6c4d{/,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30e2be33{/api,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a715278{/jobs/job/kill,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64653316{/stages/stage/kill,null,AVAILABLE,@Spark}
21/08/17 05:40:42 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bigdataanalytics2-worker-shdpt-v31-1-0.novalocal:4040
21/08/17 05:40:42 INFO executor.Executor: Starting executor ID driver on host localhost
21/08/17 05:40:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39322.
21/08/17 05:40:42 INFO netty.NettyBlockTransferService: Server created on bigdataanalytics2-worker-shdpt-v31-1-0.novalocal:39322
21/08/17 05:40:42 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/08/17 05:40:42 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bigdataanalytics2-worker-shdpt-v31-1-0.novalocal, 39322, None)
21/08/17 05:40:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager bigdataanalytics2-worker-shdpt-v31-1-0.novalocal:39322 with 366.3 MB RAM, BlockManagerId(driver, bigdataanalytics2-worker-shdpt-v31-1-0.novalocal, 393
22, None)
21/08/17 05:40:42 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bigdataanalytics2-worker-shdpt-v31-1-0.novalocal, 39322, None)
21/08/17 05:40:42 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, bigdataanalytics2-worker-shdpt-v31-1-0.novalocal, 39322, None)
21/08/17 05:40:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@331ed150{/metrics/json,null,AVAILABLE,@Spark}
21/08/17 05:40:43 INFO internal.SharedState: loading hive config file: file:/spark2.4/conf/hive-site.xml
21/08/17 05:40:43 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/apps/spark/warehouse').
21/08/17 05:40:43 INFO internal.SharedState: Warehouse path is '/apps/spark/warehouse'.
21/08/17 05:40:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3339f11c{/SQL,null,AVAILABLE,@Spark}
21/08/17 05:40:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7edcc7fb{/SQL/json,null,AVAILABLE,@Spark}
21/08/17 05:40:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1242e5ea{/SQL/execution,null,AVAILABLE,@Spark}
21/08/17 05:40:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dfa8f99{/SQL/execution/json,null,AVAILABLE,@Spark}
21/08/17 05:40:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63ebe88c{/static/sql,null,AVAILABLE,@Spark}
21/08/17 05:40:43 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/08/17 05:40:44 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
21/08/17 05:40:44 INFO datasources.InMemoryFileIndex: It took 89 ms to list leaf files for 1 paths.
21/08/17 05:40:46 INFO streaming.CheckpointFileManager: Writing atomically to hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/metadata using temp file hdfs://bigdataanalytics2-
head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/.metadata.42a1a133-48fb-45a6-af47-19bc3b29b409.tmp
21/08/17 05:40:46 INFO streaming.CheckpointFileManager: Renamed temp file hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/.metadata.42a1a133-48fb-45a6-af47-19bc3b29b409.tmp to
hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/metadata
21/08/17 05:40:46 INFO streaming.MicroBatchExecution: Starting [id = 9b8a4b76-4ccc-47af-bc10-941524f88e66, runId = 65f0fd1e-9952-45bb-9049-5fa0442d8a17]. Use hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/
tmp/checkpoint/20210817054045 to store the query checkpoint.
21/08/17 05:40:46 INFO streaming.FileStreamSourceLog: Set the compact interval to 10 [defaultCompactInterval: 10]
21/08/17 05:40:46 INFO streaming.FileStreamSource: maxFilesPerBatch = None, maxFileAgeMs = 604800000
21/08/17 05:40:46 INFO streaming.MicroBatchExecution: Using Source [FileStreamSource[hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/process_csv_as_stream]] from DataSourceV1 named 'FileSource[process_csv_a
s_stream]' [DataSource(org.apache.spark.sql.SparkSession@5964e3b7,csv,List(),Some(StructType(StructField(product_category_name,StringType,true), StructField(product_category_name_english,StringType,true))),List(),None,Map(header -> tru
e, path -> process_csv_as_stream),None)]
21/08/17 05:40:46 INFO streaming.MicroBatchExecution: Starting new streaming query.
21/08/17 05:40:46 INFO streaming.MicroBatchExecution: Stream started from {}
21/08/17 05:40:46 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
21/08/17 05:40:46 INFO streaming.CheckpointFileManager: Writing atomically to hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/sources/0/0 using temp file hdfs://bigdataanalytic
s2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/sources/0/.0.04e59e9b-289e-49f8-804c-d262b59178c5.tmp
21/08/17 05:40:46 INFO streaming.CheckpointFileManager: Renamed temp file hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/sources/0/.0.04e59e9b-289e-49f8-804c-d262b59178c5.tmp
to hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/sources/0/0
21/08/17 05:40:46 INFO streaming.FileStreamSource: Log offset set to 0 with 1 new files
21/08/17 05:40:46 INFO streaming.CheckpointFileManager: Writing atomically to hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/offsets/0 using temp file hdfs://bigdataanalytics2
-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/offsets/.0.e2a0d216-7026-4fb2-95fb-c3ce000eadac.tmp
21/08/17 05:40:46 INFO streaming.CheckpointFileManager: Renamed temp file hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/offsets/.0.e2a0d216-7026-4fb2-95fb-c3ce000eadac.tmp to
 hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/offsets/0
21/08/17 05:40:46 INFO streaming.MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1629178846442,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBack
edStateStoreProvider, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200)
)
21/08/17 05:40:46 INFO streaming.FileStreamSource: Processing 1 files from 0:0
21/08/17 05:40:46 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
21/08/17 05:40:46 INFO datasources.FileSourceStrategy: Pruning directories with:
21/08/17 05:40:46 INFO datasources.FileSourceStrategy: Post-Scan Filters:
21/08/17 05:40:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<product_category_name: string, product_category_name_english: string>
21/08/17 05:40:46 INFO execution.FileSourceScanExec: Pushed Filters:
21/08/17 05:40:47 INFO codegen.CodeGenerator: Code generated in 193.338499 ms
21/08/17 05:40:47 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 321.9 KB, free 366.0 MB)
21/08/17 05:40:47 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.0 KB, free 366.0 MB)
21/08/17 05:40:47 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on bigdataanalytics2-worker-shdpt-v31-1-0.novalocal:39322 (size: 29.0 KB, free: 366.3 MB)
21/08/17 05:40:47 INFO spark.SparkContext: Created broadcast 0 from start at NativeMethodAccessorImpl.java:0
21/08/17 05:40:47 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
START BATCH LOADING. TIME = 20210817054047
21/08/17 05:40:48 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21/08/17 05:40:48 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/08/17 05:40:48 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/08/17 05:40:48 INFO codegen.CodeGenerator: Code generated in 42.912004 ms
21/08/17 05:40:48 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
21/08/17 05:40:48 INFO scheduler.DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
21/08/17 05:40:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
21/08/17 05:40:48 INFO scheduler.DAGScheduler: Parents of final stage: List()
21/08/17 05:40:48 INFO scheduler.DAGScheduler: Missing parents: List()
21/08/17 05:40:48 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
21/08/17 05:40:48 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 178.4 KB, free 365.8 MB)
21/08/17 05:40:48 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 68.8 KB, free 365.7 MB)
21/08/17 05:40:48 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on bigdataanalytics2-worker-shdpt-v31-1-0.novalocal:39322 (size: 68.8 KB, free: 366.2 MB)
21/08/17 05:40:48 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1184
21/08/17 05:40:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
21/08/17 05:40:48 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/08/17 05:40:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 8331 bytes)
21/08/17 05:40:48 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
21/08/17 05:40:48 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/08/17 05:40:48 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21/08/17 05:40:48 INFO codec.CodecConfig: Compression: SNAPPY
21/08/17 05:40:48 INFO codec.CodecConfig: Compression: SNAPPY
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Dictionary is on
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Validation is off
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
21/08/17 05:40:48 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
21/08/17 05:40:48 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "product_category_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_category_name_english",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_date",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary product_category_name (UTF8);
  optional binary product_category_name_english (UTF8);
  required binary p_date (UTF8);
}


21/08/17 05:40:48 INFO compress.CodecPool: Got brand-new compressor [.snappy]
21/08/17 05:40:49 INFO datasources.FileScanRDD: Reading File path: hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/process_csv_as_stream/raw_csv_data.csv, range: 0-25589, partition values: [empty row]
21/08/17 05:40:49 INFO codegen.CodeGenerator: Code generated in 19.468184 ms
21/08/17 05:40:49 INFO codegen.CodeGenerator: Code generated in 11.510271 ms
21/08/17 05:40:49 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 29941
21/08/17 05:40:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210817054048_0000_m_000000_0' to hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/file_output/p_date=20210817054047/_tem
porary/0/task_20210817054048_0000_m_000000
21/08/17 05:40:49 INFO mapred.SparkHadoopMapRedUtil: attempt_20210817054048_0000_m_000000_0: Committed
21/08/17 05:40:49 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2354 bytes result sent to driver
21/08/17 05:40:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 963 ms on localhost (executor driver) (1/1)
21/08/17 05:40:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
21/08/17 05:40:49 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.091 s
21/08/17 05:40:49 INFO scheduler.DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.135964 s
21/08/17 05:40:49 INFO datasources.FileFormatWriter: Write Job 11609687-68b3-482b-9070-a2615b8a206f committed.
21/08/17 05:40:49 INFO datasources.FileFormatWriter: Finished processing stats for write job 11609687-68b3-482b-9070-a2615b8a206f.
FINISHED BATCH LOADING. TIME = 20210817054047
21/08/17 05:40:49 INFO streaming.CheckpointFileManager: Writing atomically to hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/commits/0 using temp file hdfs://bigdataanalytics2
-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/commits/.0.c352a65b-6287-4cc3-bd0c-ed1cac53011c.tmp
21/08/17 05:40:49 INFO streaming.CheckpointFileManager: Renamed temp file hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/commits/.0.c352a65b-6287-4cc3-bd0c-ed1cac53011c.tmp to
 hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/tmp/checkpoint/20210817054045/commits/0
21/08/17 05:40:49 INFO streaming.MicroBatchExecution: Streaming query made progress: {
  "id" : "9b8a4b76-4ccc-47af-bc10-941524f88e66",
  "runId" : "65f0fd1e-9952-45bb-9049-5fa0442d8a17",
  "name" : null,
  "timestamp" : "2021-08-17T05:40:46.355Z",
  "batchId" : 0,
  "numInputRows" : 1000,
  "processedRowsPerSecond" : 296.2085308056872,
  "durationMs" : {
    "addBatch" : 2827,
    "getBatch" : 99,
    "getOffset" : 76,
    "queryPlanning" : 252,
    "triggerExecution" : 3376,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/process_csv_as_stream]",
    "startOffset" : null,
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 1000,
    "processedRowsPerSecond" : 296.2085308056872
  } ],
  "sink" : {
    "description" : "ForeachBatchSink"
  }
}
21/08/17 05:40:50 INFO datasources.InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
21/08/17 05:40:50 INFO streaming.MicroBatchExecution: Streaming query made progress: {
  "id" : "9b8a4b76-4ccc-47af-bc10-941524f88e66",
  "runId" : "65f0fd1e-9952-45bb-9049-5fa0442d8a17",
  "name" : null,
  "timestamp" : "2021-08-17T05:40:50.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 8,
    "triggerExecution" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/process_csv_as_stream]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "ForeachBatchSink"
  }
}
21/08/17 05:41:00 INFO datasources.InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
21/08/17 05:41:00 INFO streaming.MicroBatchExecution: Streaming query made progress: {
  "id" : "9b8a4b76-4ccc-47af-bc10-941524f88e66",
  "runId" : "65f0fd1e-9952-45bb-9049-5fa0442d8a17",
  "name" : null,
  "timestamp" : "2021-08-17T05:41:00.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 11,
    "triggerExecution" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/process_csv_as_stream]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "ForeachBatchSink"
  }
}
21/08/17 05:41:10 INFO datasources.InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
21/08/17 05:41:10 INFO streaming.MicroBatchExecution: Streaming query made progress: {
  "id" : "9b8a4b76-4ccc-47af-bc10-941524f88e66",
  "runId" : "65f0fd1e-9952-45bb-9049-5fa0442d8a17",
  "name" : null,
  "timestamp" : "2021-08-17T05:41:10.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 11,
    "triggerExecution" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[hdfs://bigdataanalytics2-head-shdpt-v31-1-0.novalocal:8020/user/student782_3/process_csv_as_stream]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "ForeachBatchSink"
  }
}
^CTraceback (most recent call last):
  File "/home/student782_3/7.3.spark_submit_infinite_stream.py", line 52, in <module>
    stream.awaitTermination()
  File "/spark2.4/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 103, in awaitTermination
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1255, in __call__
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 985, in send_command
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1152, in send_command
  File "/usr/lib64/python2.7/socket.py", line 447, in readline
    data = self._sock.recv(self._rbufsize)
  File "/spark2.4/python/lib/pyspark.zip/pyspark/context.py", line 270, in signal_handler
KeyboardInterrupt
21/08/17 05:41:12 INFO spark.SparkContext: Invoking stop() from shutdown hook
21/08/17 05:41:12 INFO server.AbstractConnector: Stopped Spark@241cb450{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
21/08/17 05:41:12 INFO ui.SparkUI: Stopped Spark web UI at http://bigdataanalytics2-worker-shdpt-v31-1-0.novalocal:4040
21/08/17 05:41:12 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/08/17 05:41:12 INFO memory.MemoryStore: MemoryStore cleared
21/08/17 05:41:12 INFO storage.BlockManager: BlockManager stopped
21/08/17 05:41:12 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
21/08/17 05:41:12 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/08/17 05:41:12 INFO spark.SparkContext: Successfully stopped SparkContext
21/08/17 05:41:12 INFO util.ShutdownHookManager: Shutdown hook called
21/08/17 05:41:12 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-53da2276-df54-4da0-b2f0-1b17a1ae27d1
21/08/17 05:41:12 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0f44fb0d-269a-4d66-9954-74ef03fb7ce6
21/08/17 05:41:12 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0f44fb0d-269a-4d66-9954-74ef03fb7ce6/pyspark-edd4d1cf-dee0-4a06-ae34-3c10ad2dc60e
[student782_3@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$
